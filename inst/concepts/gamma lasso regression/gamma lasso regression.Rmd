---
title: "Untitled"
author: "OEB"
date: "December 5, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Regression Feature Selection

To my understanding Feature Selection for regression was pretty straigth forward using stepwise regression. However I recently learned that this approach is [flawed](https://en.wikipedia.org/wiki/Stepwise_regression). The alternatively used method should be the Lasso. However most of the response variables I do regression for are not normally distributed and have a distribution that looks to my like a gamma or beta distribution. I recently had good results fitting a gamma regression model, beta distribution models did usually not converge to a minimum or did not improve the fit. It is however challenging to find an implementation of Lasso that works for a gamma regression. 

# R Implementations for Lasso

- `glmnet` does not support gamma regression but is the most used lasso implementation. [Documenatation](http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)
- `HDtweedie` a gamma regression is a [tweedie distribution](https://en.wikipedia.org/wiki/Tweedie_distribution) for `p == 2`, the package offers lasso for those distributions. [documentation](https://jasdumas.github.io/tech-short-papers/HDtweedie_lasso_tutorial.html)
- `gamlss` offers Lasso and gamma regression. The way the predict.gamlss is implemented (expects fitting data to be present in global environment when calling predict) makes it difficult to integrate in functions. There also is no documentation.  

# Example Data

In order to test the Lasso implementations we should find or simulate a dataset with a gamma distributed response variable

## The quine data set
```{r}
t = tibble( rate = seq(0.1,5, 0.05) 
            , shape = list(seq(0.1,5, 0.1) ) 
)%>%
  unnest(shape)%>%
  mutate( test = map2( rate, shape, function(x,y)  ks.test( MASS::quine$Days, 'pgamma', x, y ) ) 
          ,p_val = map_dbl(test, 'p.value')
          ,D = map_dbl(test,'statistic')
  ) %>%
  select(-test) 

summary(t)

t %>%
  filter(p_val <= 0.0001 ) %>%
  filter(D == min(D) )

hist( scale(MASS::quine$Days, center = T) )
hist( scale(rgamma(146,0.2,2.65), center = T ) )
```

# HDtweedie

## Quine Data

```{r}

require(oetteR)
require(pipelearner)
require(tidyverse)


data = MASS::quine %>%
  as_tibble()

formula = Days~.

grid = 10^seq(4,-4,length= 100)

call_cont = make_container_for_function_calls()
call_cont$set_total(600)

lasso = function( data, formula, grid, call_cont){

  wr_tweedie = function(data, formula, lambda, p_fact ){
  
    response_var = f_manip_get_response_variable_from_formula(formula)
  
    y = data[[response_var]]
    x = model.matrix(formula, data)[,-1]
  
    m = HDtweedie::HDtweedie(x,y, lambda = lambda, p = p_fact, alpha = 1 )
  
  }
  
  wr_glmnet = function(data, formula, lambda ){
  
    response_var = f_manip_get_response_variable_from_formula(formula)
  
    y = data[[response_var]]
    x = model.matrix(formula, data)[,-1]
  
    m =glmnet::glmnet(x,y, lambda = lambda, alpha = 1 )
  
  }
  
  
  pl = pipelearner(data) %>%
    learn_models(models = c(call_cont$make_call)
                 , formulas = c(formula)
                 , .f = c(wr_tweedie)
                 , function_name = c('tweedie')
                 , lambda = grid
                 , p_fact = c(1,1.5,2)
                 ) %>%
    learn_models(models = c(call_cont$make_call)
                 , formulas = c(formula)
                 , .f = c(wr_glmnet)
                 , function_name = c('glmnet')
                 , lambda = grid
    ) %>%
    learn_cvpairs( crossv_mc, n = 1, test = 0.01 ) %>%
    learn()
  
  pl = pl %>%
    mutate(  lambda = map_dbl(params, 'lambda')
            , function_name = map_chr(params, 'function_name')
            )
  
  pl_glm = pl %>%
    filter( function_name == 'glmnet') %>%
    mutate( p = -1
            , coef = map(fit, coef )
            , coef = map(coef, as.matrix )
            , coef = map(coef, as.data.frame )
            , coef = map(coef, function(x) mutate(x, coef = row.names(x))  )
    )
  
  pl_tweedie = pl %>%
    filter( function_name == 'tweedie') %>%
    mutate( p = map_dbl(params, 'p_fact')
            , coef = map(fit, coef )
            , coef = map(coef, as.matrix )
            , coef = map(coef, as.data.frame )
            , coef = map(coef, function(x) mutate(x, coef = row.names(x))  )
    )
  
  pl_all = pl_glm %>%
    bind_rows(pl_tweedie) %>%
    mutate( title = paste(function_name, lambda, p))
  
  return(pl_all)
  
}


plot_rtmse = function( pl_all, newdata = 'train'){

  pl_pred = pl_all %>%
    f_predict_pl_regression(  formula = formula, newdata = newdata) %>%
    unnest(preds, .drop = F)
  
  pl_lab = pl_pred %>%
    group_by(title, lambda, p, models.id) %>%
    summarize()
  
  pl_sum = pl_pred %>%
    f_predict_pl_regression_summarize() %>%
    left_join( pl_lab )
  
  p = ggplot(pl_sum, aes( log(lambda)
                      , rtmse
                      , fill = as.factor(p)
                      , color = as.factor(p)
                      )
         )+
    geom_line() +
    scale_fill_manual( values = f_plot_col_vector74() )+
    scale_color_manual( values = f_plot_col_vector74())
  
  
  return( plotly::ggplotly(p) )

}

# pl_all = lasso(data, formula, grid, call_cont)
# 
# plot_rtmse(pl_all)

```


Because the Data Set is quite small we are calculating rtmse against the training Data. Which is likely to overfit.

### Coefficients

```{r}

p_coef = pl_all %>%
  unnest(coef, .drop = F ) %>%
  group_by(coef, p, lambda) %>%
  summarize( s0 = mean(s0) )


p_coef %>%
  filter( p == -1 ) %>%
  ggplot( aes(x = log(lambda), y = s0, color = coef) ) +
  geom_line()+
  geom_point()

p_coef %>%
  filter( p == 1 ) %>%
  ggplot( aes(x = log(lambda), y = s0, color = coef) ) +
  geom_line()+
  geom_point()

p_coef %>%
  filter( p == 1.5 ) %>%
  ggplot( aes(x = log(lambda), y = s0, color = coef) ) +
  geom_line()+
  geom_point()



```


# Simulated Data

```{r}

data = tibble( resp = rgamma(5000,1) ) %>%
  mutate( exp1 = rnorm(5000, 50,5)
          , exp2 = resp +500 /rnorm(5000) * 22 
          , exp3 = runif(5000,1,100)
          , exp4 = resp * rnorm(5000)
          )

data_res = select(data, resp)
data_exp = select(data, -resp) %>%
  mutate_all(scale, center = T)

data_scales = data_res %>%
  bind_cols(data_exp)

formula = resp~.


pl_all = lasso(data_scales, formula, grid, call_cont)

plot_rtmse(pl_all, newdata = 'test')



```

## Coefficients

```{r}

p_coef = pl_all %>%
  unnest(coef, .drop = F ) %>%
  group_by(coef, p, lambda) %>%
  summarize( s0 = mean(s0) )


p_coef %>%
  filter( p == -1 ) %>%
  ggplot( aes(x = log(lambda), y = s0, color = coef) ) +
  geom_line()+
  geom_point()

p_coef %>%
  filter( p == 1.5 ) %>%
  ggplot( aes(x = log(lambda), y = s0, color = coef) ) +
  geom_line()+
  geom_point()

p_coef %>%
  filter( p == 2 ) %>%
  ggplot( aes(x = log(lambda), y = s0, color = coef) ) +
  geom_line()+
  geom_point()

```


For some reason the `glmnet` algorithm discards the variables exp1,exp3,exp4 to begin with and thus builds a better modell since exp2 seems to be the strongests predictor variable. There does not seem to be any obvious advantage of using tweedie distributions so far.

## get minimum for lambda

```{r}

pl_pred = pl_all %>%
  f_predict_pl_regression(  formula = formula, newdata = 'test') %>%
  unnest(preds, .drop = F)

pl_lab = pl_pred %>%
  group_by(title, lambda, p, models.id) %>%
  summarize()

pl_sum = pl_pred %>%
  f_predict_pl_regression_summarize() %>%
  left_join( pl_lab ) %>%
  group_by(p) %>%
  filter( rtmse == min(rtmse) ) %>%
  select(p, lambda)

pl_min_lambda = pl_sum %>%
  left_join(p_coef)

```

## fit models

```{r}

wr_tweedie = function(data, formula, lambda, p_fact ){

  response_var = f_manip_get_response_variable_from_formula(formula)

  y = data[[response_var]]
  x = model.matrix(formula, data)[,-1]

  m = HDtweedie::HDtweedie(x,y, lambda = lambda, p = p_fact, alpha = 1 )

}


p_mod = pl_min_lambda %>%
  filter( coef != '(Intercept)', s0 > 0 ) %>%
  group_by( p ) %>%
  summarise( formula = paste(coef, collapse = '+') ) %>%
  mutate( formula = paste('resp~', formula)
          , formula = map(formula, as.formula) 
          )

require(statmod)

p_mod_lm = p_mod %>%
  filter( p == - 1 ) %>%
  mutate( m = map(formula, lm, data) )

p_mod_tweed = p_mod %>%
  filter( p %in% c(1,1.5) ) %>%
  mutate( m = map2(formula
                  , p
                  , function(x,y) glm( x, data, family = tweedie(var.power = y) ) ) 
          )

p_mod_gamlss = p_mod %>%
  filter( p == 2 ) %>%
  mutate( m = map(formula
                  , gamlss::gamlss
                  , data = data
                  , family = gamlss.dist::GA() 
                  )                    
          ) 


wr_predict = function(m){
  

  
}

p_mod_pred = p_mod_lm %>%
  bind_rows(p_mod_tweed) %>%
  bind_rows(p_mod_gamlss) %>%
  mutate( preds = map( m, predict)
          , preds = map( preds, function(x) tibble( preds = x) ) 
          , data = list(data) 
          , preds = map2(preds, data, bind_cols ) ) %>%
  select(-data, -formula) %>%
  unnest(preds, .drop = F) %>%
  mutate( rtmse = preds-resp 
          ,rtmse = rtmse^2 ) %>%
  group_by(p) %>%
  summarise( mean_rtmse = sqrt( mean(rtmse) ) )


m_lm = p_mod_lm$m[[1]]
summary(m_lm)
plot(m_lm)

m_twe = p_mod_tweed$m[[1]]
summary(m_twe)
plot(m_twe)

m_ga = p_mod_gamlss$m[[1]]
summary(m_ga)
plot(m_ga)


```

