---
title: "Untitled"
author: "OEB"
date: "December 5, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
require(oetteR)
require(pipelearner)
require(tidyverse)

```


# Regression Feature Selection

To my understanding Feature Selection for regression was pretty straigth forward using stepwise regression. However I recently learned that this approach is [flawed](https://en.wikipedia.org/wiki/Stepwise_regression). The alternatively used method should be the Lasso. However most of the response variables I do regression for are not normally distributed and have a distribution that looks to my like a gamma or beta distribution. I recently had good results fitting a gamma regression model, beta distribution models did usually not converge to a minimum or did not improve the fit. It is however challenging to find an implementation of Lasso that works for a gamma regression. 

# R Implementations for Lasso

- `glmnet` does not support gamma regression but is the most used lasso implementation. [Documenatation](http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)
- `HDtweedie` a gamma regression is a [tweedie distribution](https://en.wikipedia.org/wiki/Tweedie_distribution) for `p == 2`, the package offers lasso for those distributions. [documentation](https://jasdumas.github.io/tech-short-papers/HDtweedie_lasso_tutorial.html)
- `gamlss` offers Lasso and gamma regression. The way the predict.gamlss is implemented (expects fitting data to be present in global environment when calling predict) makes it difficult to integrate in functions. There also is no documentation.  

# `oetteR::f_train_lasso`

I have written a wrapper function for `glmnet` and `HDtweedie` which is implemented in `oetteR`. The Implementation should make it easy to judge the feature reducing qualities of the lasso for different datasets.

When building a model with a specific lambda value, the coefficients will vary for each cross validation pair. When I built my coefficient plot I have to calculate the mean of all coefficients for a specific lambda value. 

It would make sense to me if one would have a fixed value for the coefficients that would be tested against different cross validation pairs. I am also not sure whether the reduced set of variables that can be recommended by the Lasso algorithm is reliant on the exact values of the coefficients that come with it. My understanding is, that a new model, fit using the same variables as recommended, should result in a fit of similar quality. 

A next step could be to use the cross validation implementations that come with those packages. I expect those algorithms to be much faster than my cross validation approach based on `pipelearner`


# Simulated Data

```{r}

data = tibble( resp = rgamma(5000,1) ) %>%
  mutate( exp1 = rnorm(5000, 50,5)
          , exp2 = log(resp) ##non linear relationship
          , exp3 = exp2 * rnorm(5000, 1, 0.5) # colinearity to exp2
          , exp4 = resp * rgamma(5000, 0.5)
          , exp5 = resp * rgamma(5000, 0.5) * rnorm(5000, 1, 0.5)
  )


form = resp~exp1+exp2+exp3+exp4+exp5


lasso = f_train_lasso(data, form
                      , grid = 10^seq(-3,3,length.out = 25)
                      , p = c(1,1.25,1.5,1.75,2)
                      , k = 10
                      )

f_datatable_universal(lasso$formula_as_tib)

```

```{r}
plotly::ggplotly(lasso$plot_rtmse)
```

Here we can see that the overall rtmse is lower for models fit with the tweedie distribution. We can however see that the minimum for all distributions is at the smallest lambda value for the tweedie distributions and at a very low value for the gaussian distribution. The lasso algorithm performs a initial fit to obtain the starting values for the coefficients, then the values of the coefficients are gradually decreases as lambda increases. We can see that the initial fit for the tweedie distributions already represents a near perfect fit and that overfitting effects do not decrease if the coefficients shrink. We find that for the gaussian distribution the initial fit contains an overfitting error that can be slightly reduced if the coefficients are decreased. 

```{r}
plotly::ggplotly(lasso$plot_coef)
```

In this plot we see the coefficient values and how they respond to increasing lambda values. The vertical line shows the lambda value at which we find the lowest cross validated rtmse value. 

# Auto Data

This Dataset comes with the HDtweedie package, the response variable has a typical tweedie p:1.5 distribution.

```{r}
library(HDtweedie)
data('auto')


data = tibble( resp = auto$y ) %>%
  bind_cols( as_tibble( scale(auto$x) ) ) 

form = paste( 'resp~', paste( colnames(auto$x), collapse = '+' ) ) %>%
  as.formula()


lasso = f_train_lasso(data, form
                      , grid = 10^seq(-3,3,length.out = 25)
                      , p = c(1,1.5,2)
                      , k = 5
                      )

f_datatable_universal(lasso$formula_as_tib)

```

```{r}
plotly::ggplotly(lasso$plot_rtmse)
```

```{r}
plotly::ggplotly(lasso$plot_coef)
```


