---
title: "Untitled"
author: "OEB"
date: "December 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Regression Feature Selection

To my understanding Feature Selection for regression was pretty straigth forward using stepwise regression. However I recently learned that this approach is [flawed](https://en.wikipedia.org/wiki/Stepwise_regression). The alternatively used method should be the Lasso. However most of the response variables I do regression for are not normally distributed and have a distribution that looks to my like a gamma or beta distribution. I recently had good results fitting a gamma regression model, beta distribution models did usually not converge to a minimum or did not improve the fit. It is however challenging to find an implementation of Lasso that works for a gamma regression. 

# R Implementations for Lasso

- `glmnet` does not support gamma regression but is the most used lasso implementation. [Documenatation](http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)
- `HDtweedie` a gamma regression is a [tweedie distribution](https://en.wikipedia.org/wiki/Tweedie_distribution) for `p == 2`, the package offers lasso for those distributions. [documentation](https://jasdumas.github.io/tech-short-papers/HDtweedie_lasso_tutorial.html)
- `gamlss` offers Lasso and gamma regression. The way the predict.gamlss is implemented (expects fitting data to be present in global environment when calling predict) makes it difficult to integrate in functions. There also is no documentation.  

# Example Data

In order to test the Lasso implementations we should find or simulate a dataset with a gamma distributed response variable

## The quine data set
```{r}
t = tibble( rate = seq(0.1,5, 0.05) 
            , shape = list(seq(0.1,5, 0.1) ) 
)%>%
  unnest(shape)%>%
  mutate( test = map2( rate, shape, function(x,y)  ks.test( MASS::quine$Days, 'pgamma', x, y ) ) 
          ,p_val = map_dbl(test, 'p.value')
          ,D = map_dbl(test,'statistic')
  ) %>%
  select(-test) 

summary(t)

t %>%
  filter(p_val <= 0.0001 ) %>%
  filter(D == min(D) )

hist( scale(MASS::quine$Days, center = T) )
hist( scale(rgamma(146,0.2,2.65), center = T ) )
```

# HDtweedie

## Quine Data

```{r}
require(oetteR)
require(pipelearner)
require(tidyverse)


data = MASS::quine %>%
  as_tibble()

formula = Days~.

grid = 10^seq(10,-4,length= 10)

call_cont = make_container_for_function_calls()
call_cont$set_total(600)

wr_tweedie = function(data, formula, lambda, p_fact ){

  response_var = f_manip_get_response_variable_from_formula(formula)

  y = data[[response_var]]
  x = model.matrix(formula, data)[,-1]

  m = HDtweedie::HDtweedie(x,y, lambda = lambda, p = p_fact )

}

wr_glmnet = function(data, formula, lambda ){

  response_var = f_manip_get_response_variable_from_formula(formula)

  y = data[[response_var]]
  x = model.matrix(formula, data)[,-1]

  m =glmnet::glmnet(x,y, lambda = lambda, alpha = 1 )

}


pl = pipelearner(data) %>%
  learn_models(models = c(call_cont$make_call)
               , formulas = c(formula)
               , .f = c(wr_tweedie)
               , function_name = c('tweedie')
               , lambda = grid
               , p_fact = seq(1,2,0.1)
               ) %>%
  learn_models(models = c(call_cont$make_call)
               , formulas = c(formula)
               , .f = c(wr_glmnet)
               , function_name = c('glmnet')
               , lambda = grid
  ) %>%
  learn_cvpairs( crossv_mc, n = 1, test = 0.01 ) %>%
  learn()

pl = pl %>%
  mutate(  lambda = map_dbl(params, 'lambda')
          , function_name = map_chr(params, 'function_name')
          )

pl_glm = pl %>%
  filter( function_name == 'glmnet') %>%
  mutate( p = -1
          , coef = map(fit, coef )
          , coef = map(coef, as.matrix )
          , coef = map(coef, as.data.frame )
          , coef = map(coef, function(x) mutate(x, coef = row.names(x))  )
  )

pl_tweedie = pl %>%
  filter( function_name == 'tweedie') %>%
  mutate( p = map_dbl(params, 'p_fact')
          , coef = map(fit, coef )
          , coef = map(coef, as.matrix )
          , coef = map(coef, as.data.frame )
          , coef = map(coef, function(x) mutate(x, coef = row.names(x))  )
  )

pl_all = pl_glm %>%
  bind_rows(pl_tweedie) %>%
  mutate( title = paste(function_name, lambda, p))

pl_pred = pl_all %>%
  f_predict_pl_regression(  formula = formula, newdata = 'train')%>%
  unnest(preds, .drop = F)

pl_lab = pl_pred %>%
  group_by(title, lambda, p, models.id) %>%
  summarize()

pl_sum = pl_pred %>%
  f_predict_pl_regression_summarize() %>%
  left_join( pl_lab )

p = ggplot(pl_sum, aes( log(lambda)
                    , rtmse
                    , fill = as.factor(p)
                    , color = as.factor(p)
                    )
       )+
  geom_line() +
  geom_point() +
  scale_fill_manual( values = f_plot_col_vector74() )+
  scale_color_manual( values = f_plot_col_vector74())


plotly::ggplotly(p)

```

