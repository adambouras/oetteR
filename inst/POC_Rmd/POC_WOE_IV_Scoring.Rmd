---
title: "WOE, IV and Scorevalues"
author: "OEB"
date: "March 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```


```{r libs}

suppressPackageStartupMessages( require(scorecard) )
suppressPackageStartupMessages( require(tidyverse) )
suppressPackageStartupMessages( require(oetteR) )

```


# Introduction

Weight of evidence (WOE), Information Value (IV) and Score values are common terms when you encounter credit risk modelling in the financial industry. Basically these terms describe certain practices when modelling credit risk with logistic regression and they have been around since the 1950. These techniques do not come up in more modern statistic books when teaching logistic regression, so I wonder what their advantages are. So I read up on them.

## Sources

### Low level introduction to WOE and IV
- http://ucanalytics.com/blogs/data-visualization-case-study-banking/
- http://ucanalytics.com/blogs/data-visualization-case-study-banking-part-2/
- http://ucanalytics.com/blogs/case-study-example-banking-logistic-regression-3/
- http://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/

### Scoring
- http://ucanalytics.com/blogs/credit-scorecards-advanced-analytics-part-4/

### Introduction to WOE and IV and a complementary R package
- https://multithreaded.stitchfix.com/blog/2015/08/13/weight-of-evidence/


## Weight of evidence

This is basically a technique that can be applied if we have a binary response variable and any kind of predictor variable. We replace the response variable values with t

TODO finish

## Information Value

```{r}

tribble( ~`Information Value`, ~`Predictive Power`
        , '< 0.02'          , 'useless for prediction'
        , '0.02 - 0.1'      , 'weak predictor'
        , '0.1 - 0.3'       , 'medium predictor'
        , '0.3 - 0.5'       , 'strong predictor'
        , '> 0.5'           , 'suspicious too good to be true') %>%
  knitr::kable( align = c('cl') )

```

# WOE, IF, Scorecards implementation in R

package: `scorecard`

```{r}

data('germancredit')

data = germancredit %>%
  as_tibble()

#replace '.' in variable names not compatible with f_train_lasso
vars = names(data) %>%
  str_replace_all( '\\.', '_')

names(data) <- vars

summary(data)

```

# Missing Data

No mising data in dataset

```{r}

Amelia::missmap(data)

```


# Select variables using information value

```{r}

iv = iv(data, y = 'creditability') %>%
  as_tibble() %>%
  mutate( info_value = round(info_value, 3) ) %>%
  arrange( desc(info_value) )

iv %>%
  knitr::kable()


vars = iv %>%
  filter( info_value >= 0.02 )


```


# Weight of evidence binning

```{r}

bins = woebin(data, y = 'creditability')


```

## Examplatory Plot

```{r}

bins$duration_in_month %>%
  knitr::kable()

woebin_plot(bins$duration_in_month)

```

# Apply bins

```{r}

data_woe = woebin_ply( data, bins ) %>%
  as_tibble()

```


# glm with lasso and crossvalidataion

```{r}

formula = as.formula( paste( 'creditability ~', paste(vars, collapse = '+') ) )


lasso = oetteR::f_train_lasso( data = data_woe
                               , p = NULL
                               , formula = trans_ls$formula
                               , k = 10
                               , family = 'binomial'
                               )


```

## Lasso vis 

```{r}

lasso$plot_mse

lasso$plot_coef

lasso$formula_str_lambda_1se

lasso$tib_all %>%
  filter(lambda == lambda_1se) %>%
  select( lambda_1se, auc, n_coeff_before_lasso, n_coeff_after_lasso)

```

We can eliminate 6 out of 20 variables applying the lasso.


# Build and interpret the model

Formula logistic regression :

$$ln\left(\frac{P(X)}{1-P(X)}\right) = intercept + \beta_1x + \beta_nx $$
the term on the left is called the link function its result is the **logit** value

We can convert the **logit** value to **odds** by $$e^{logit}$$

The **odds** can be converted to probability *P* by $$P(X)=\frac{odds}{odds+1}$$

Classically we woul use `predict()` with `type = 'response'` to directly get the porbability. However here we will do the calculations manually as described above.

```{r}

formula = as.formula( lasso$formula_str_lambda_1se )

m = glm( formula, data_woe, family = 'binomial')


pred = predict(m)
resp = predict(m, type = 'response')

res = tibble( logit = pred
              , odds = exp(pred)
              , prob = odds / (odds + 1)
              , resp = resp )


res %>%
  head(10) %>%
  knitr::kable()

```

# Score Card

We can use `scorecard::scorecard()` in order to convert the **logit** to a contineous value.

```{r}

card = scorecard( bins , m
                  , points0 = 600 ## target maximum score
                  , pdo = 50 ## score points to double odds 
                  )

sc = scorecard_ply( data, card )

res$score = sc[[1]]

```

# Plot Scores and Probabilities

```{r}

res
  


```







