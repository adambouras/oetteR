---
title: "regression with `caret`"
author: "OEB"
date: "December 29, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r results = 'hide'}

suppressPackageStartupMessages( require(oetteR) )
suppressPackageStartupMessages( require(tidyverse) )
suppressPackageStartupMessages( require(caret) )

```


We will try out several regression models supported by caret and presented in `Applied Predictive Modelling`  

- robust linear regression
- lasso  
- elastic net  
- neuronal networks  
- mars  
- svm  

# Data

The `mlbench` package includes several simulated datasets that can be used as benchmarks

```{r}

set.seed(1)

df = tibble( data = list(mlbench::mlbench.friedman1( 1000 )
                         , mlbench::mlbench.friedman2( 1000 )
                         , mlbench::mlbench.friedman3( 1000 ) 
                        )
  ) %>%
  mutate( x = map(data, 'x')
          , y = map(data, 'y')
          , x = map( x, as_tibble )
          , y = map( y, function(z) tibble(resp = z) )
          , data = map2( y ,x, bind_cols) 
          ) %>%
  select(data) 

```

# X-Y Plots

```{r}

df_plot = df %>%
  mutate( p = map(data, gather, key = 'key', value = 'value', - resp)
          , p  = map(p, oetteR::f_plot_pretty_points, 'value','resp','key', scales = 'free_x')
          , p  = map(p, function(x) x + geom_smooth() + geom_rug() + ggpubr::stat_cor() )
          , hist = map( data, f_clean_data_no_changes )
          , hist = map( hist, function(x) list( map( x$all_variables, f_plot_hist, x) ) )
  )

df_plot$p

```


# Histograms

```{r}

df_plot = df %>%
  mutate( hist = map(data, gather, key = 'key', value = 'value')
          , hist = map( hist, f_clean_data_no_changes )
          , hist = map( hist, function(x) f_plot_hist('value', x , add = 'none')  )
          , hist = map( hist, function(x) x = x + facet_wrap(~key, scales = 'free') )
  ) 


df_plot$hist

```


# Summary

## Dataset 1
All variables are normally distributed and scaled.
- V4, V5 correlate with the response in a linear fashion
- V1, V2 correlate almost linearly but in the higher ranges adop


# robust linear regression

Regular regression models seek to minimize SSE (squared standard error). This overpenalizes large errors over small errors which makes linear regression quite sensitive to outliers. The huber method counts smaller errors as squared errors and larger errors as absolute errors making regressions more robust. ![Huber method][POC_regression_models_caret_robust_linear_regression.png]


```{r}

tribble( ~Property, ~Sensitivity
         , 'Colinearity', 'sensitive'
         , 'Outlier', 'less sensitive'
         , 'Nonlinearity', 'sensitice'
         , 'Irrelevant variables', 'sensitive'
         , 'Unscaled variables', 'unsensitive'
         ) %>%
  knitr::kable( align = 'lc')

```



```{r}

```



